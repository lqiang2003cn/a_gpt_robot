[user]
You are an excellent interpreter of human instructions for household tasks. Given an instruction and information
about the working environment, you break it down into a sequence of robot actions.
[assistant]
Waiting for next input.

[user]
Necessary and sufficient robot action list are defined as follows:
"""
"ROBOT ACTION LIST"
- move_base_to_pose(pose_str): move the robot to the pose specified by the argument pose_str
- move_arm_to_pose(pose_str): move the robot arm to the pose specified by the argument pose_str
- close_gripper(): close the gripper of the robot
- open_gripper(): open the gripper of the robot
- attach_box(): attach the object to the gripper
"""
[assistant]
Waiting for next input.

[user]
The general principles of interpreting human instructions are:
"""
- Before doing a task, move the robot arm to the holding pose and open its gripper
- To take an object from one location to another location, you need to pick it up from where it is and then place it on the specified table in the human instruction
- To pick up an object, you need to do the following things in the correct sequence:
    '''
    - move the robot to the front of the table
    - move the robot arm to the prepick pose of the object
    - move the robot arm to the pick pose of the object
    - close the robot's gripper
    - attach the object to the gripper.
    - move the robot arm to the prepick pose of the object
    - move the robot to the turn around pose
    - move the robot arm to the holding pose
    '''
- To place an object on a table, you need to do the following things in the correct sequence:
    '''
    - move the robot to the front of the table
    - move the robot arm to the preplace pose of the table
    - move the robot arm to the place pose of the table
    - open the robot's gripper
    - move the robot arm to the preplace pose of the table
    - move the robot to the turn around pose
    - move the robot arm to the holding pose
    '''
- After placing an object, move the robot to the rest spot
"""
[assistant]
Waiting for next input.

[user]
The working environment of the robot consists of three types of entities: assets, objects, markers. The following are detailed explanations:
"""
- assets: entities that cannot be moved around
- objects: entities that can be grabbed around
"""
Objects are entities that can be grabbed around. Assets are .
The states of object and assets are represented using python dictionary. Example:
"""
{
    "environment": {
        "assets": [
            "<table_1>",
            "<table_2>"
        ],
        "objects": [
            "<cube_1>"
        ],
        "object_states": {
            "<cube_1>": ["ON(<table_1>)"]
        }
    }
}
"""
Objects are represented as <object_id> and assets are represented as <asset_id>. The states of object and assets are represented using the following "STATE LIST":
"""
"STATE LIST"
- ON(<something>): Object is on top of <something>.
"""
[assistant]
Waiting for next input.



[user]
Your response should only be a json object, with no explanation or conversation. You divide the actions given in the text into detailed robot actions.
The json object has four keys.
"""
- dictionary["task_cohesion"]: A dictionary containing information about the robot's actions that have been split up.
- dictionary["environment_before"]: The environment before the manipulation.
- dictionary["environment_after"]: The environment after the manipulation.
- dictionary["question"]: If you cannot understand the given sentence, you can ask the user to rephrase the sentence. Leave this key empty if you can understand the given sentence.
"""

One key exists in dictionary["task_cohesion"].
"""
- dictionary["task_cohesion"]["task_sequence"]: contains a list of robot actions. If nothing needs to be done, just return an empty json object. Each action is a json object with two keys:
    '''
    - action_name: the name of the action. Only the actions defined in the "ROBOT ACTION LIST" will be used.
    - action_param: the parameters of the action should be a json object, for example:
    {
        'param_name1':'param_value1',
        'param_name2':'param_value2'
    }
    '''
"""
[assistant]
Waiting for next input.




[user]
I will give you an example of the input and the output you will generate. Example:
"""
- Input:
'''
{
    "environment": {
        "assets": [
            "<table_1>",
            "<table_2>"
        ],
        "objects": [
            "<box_0>"
        ],
        "object_states": {
            "<box_0>": ["ON(<table_1>)"]
        }
    },
   "instruction": "Take the box 0 to table 2"
}
'''
- Output:
'''
{
    "task_cohesion": {
        "task_sequence": [
            {
                "action_name": "move_arm_to_pose",
                "action_param":{"pose_str":"holding pose"}
            },
            {
                "action_name": "open_gripper",
                "action_param":{}
            },
            {
                "action_name": "move_base_to_pose",
                "action_param":{"position":"front of table 2"}
            },
            {
                "action_name": "move_arm_to_pose",
                "action_param":{"position":"prepick pose of box 0"}
            },
            {
                "action_name": "move_arm_to_pose",
                "action_param":{"position":"pick pose of box 0"}
            },
            {
                "action_name": "close_gripper",
                "action_param":{}
            },
            {
                "action_name": "attach_object",
                "action_param":{}
            },
            {
                "action_name": "move_arm_to_pose",
                "action_param":{"position":"prepick pose of box 0"}
            },
            {
                "action_name": "move_base_to_pose",
                "action_param":{"position":"turn around"}
            },
            {
                "action_name": "move_arm_to_pose",
                "action_param":{"position":"holding pose"}
            },
            {
                "action_name": "move_base_to_pose",
                "action_param":{"position":"front of table 2"}
            },
            {
                "action_name": "move_arm_to_pose",
                "action_param":{"position":"preplace pose of table 2"}
            },
            {
                "action_name": "move_arm_to_pose",
                "action_param":{"position":"place pose of table 2"}
            },
            {
                "action_name": "open_gripper",
                "action_param":{}
            },
            {
                "action_name": "move_arm_to_pose",
                "action_param":{"position":"preplace pose of table 2"}
            },
            {
                "action_name": "move_base_to_pose",
                "action_param":{"position":"turn around"}
            },
            {
                "action_name": "move_arm_to_pose",
                "action_param":{"position":"holding pose"}
            },
            {
                "action_name": "move_base_to_pose",
                "action_param":{"position":"rest spot"}
            }
        ]
    },
    "environment_before": {
        "assets": [
            "<table_1>",
            "<table_2>"
        ],
        "objects": [
            "<box_0>"
        ],
        "object_states": {
            "<box_0>": ["ON(<table_1>)"]
        }
    },
    "environment_after": {
        "assets": [
            "<table_1>",
            "<table_2>"
        ],
        "objects": [
            "<box_0>"
        ],
        "object_states": {
            "<box_0>": ["ON(<table_2>)"]
        }
    },
    "question":""
}
'''
"""
[assistant]
Waiting for next input.

[user]
Now start working. The input is:
{
    "environment": {
        "assets": [
            "<table_333>",
            "<table_238>"
        ],
        "objects": [
            "<box_123>"
        ],
        "object_states": {
            "<box_123>": ["ON(<table_333>)"]
        }
    },
    "instruction:": "Take the box 123 to table 238"
}
